{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# {{ cookiecutter.project_name }}\n",
    "\n",
    "{{ cookiecutter.description }}\n",
    "\n",
    "创建时间: {% now 'utc', '%Y-%m-%d %H:%M:%S' %}\n",
    "作者: {{ cookiecutter.author_name }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_processor import BatchProcessor\n",
    "from config import DB_CONFIG, DATA_CONFIG, CACHE_CONFIG\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from cachetools import cached, LRUCache\n",
    "from collections import defaultdict\n",
    "\n",
    "# 如果启用缓存，创建缓存装饰器\n",
    "if CACHE_CONFIG['enable']:\n",
    "    cache = LRUCache(maxsize=CACHE_CONFIG['size'])\n",
    "    api_cache = cached(cache)\n",
    "else:\n",
    "    # 空装饰器，不使用缓存\n",
    "    def api_cache(func):\n",
    "        return func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processor-def",
   "metadata": {},
   "source": [
    "## 2. 定义处理器类\n",
    "\n",
    "**需要实现以下3个方法:**\n",
    "1. `get_data_source()` - 定义数据源\n",
    "2. `define_schema()` - 定义表结构  \n",
    "3. `process_business_logic()` - 实现批量业务处理逻辑\n",
    "\n",
    "**可选实现:**\n",
    "4. `fetch_external_data()` - 如需要调用外部API，可重写此方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processor-class",
   "metadata": {},
   "outputs": [],
   "source": "class {{ cookiecutter.project_slug.title().replace('_', '') }}Processor(BatchProcessor):\n    \n    def get_data_source(self):\n        \"\"\"\n        定义数据源\n        \n        返回CSV文件路径或DataFrame对象\n        \"\"\"\n        return DATA_CONFIG['data_file']\n    \n    def define_schema(self) -> Dict[str, list]:\n        \"\"\"\n        定义表结构字段\n        \n        TODO: 根据你的需求修改以下字段定义\n        \"\"\"\n        return {\n            'control_fields': [\n                'is_processed',    # 处理状态(系统字段，勿删)\n                'retry_count'      # 重试次数(系统字段，勿删)\n            ],\n            'result_fields': [\n                'result1',         # TODO: 替换为你的结果字段1\n                'result2',         # TODO: 替换为你的结果字段2  \n                'result3'          # TODO: 替换为你的结果字段3\n            ]\n        }\n    \n    {% if cookiecutter.enable_cache == 'y' %}@api_cache\n    def _fetch_api_data_cached(self, query_params_tuple):\n        \"\"\"\n        带缓存的API调用方法 - 内部方法\n        \n        Args:\n            query_params_tuple: 查询参数元组(用于缓存)\n            \n        Returns:\n            API响应数据\n        \"\"\"\n        # 将元组转换回列表\n        query_params = list(query_params_tuple)\n        \n        # TODO: 替换为你的实际API调用逻辑\n        # api_url = 'https://api.example.com/your-endpoint'\n        # headers = {'Content-Type': 'application/json'}\n        # payload = json.dumps(query_params)\n        # response = requests.post(api_url, headers=headers, data=payload, timeout=30)\n        # \n        # if response.status_code == 200:\n        #     return response.json()\n        # else:\n        #     self.logger.warning(f\"API调用失败: {response.status_code}\")\n        #     return {}\n        \n        return {}{% endif %}\n    \n    def fetch_external_data(self, batch_data: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"\n        获取外部数据(如API调用) - 可选实现\n        \n        Args:\n            batch_data: 当前批次的数据\n            \n        Returns:\n            外部数据字典\n            \n        TODO: 如果需要调用外部API，请实现此方法\n        \"\"\"\n        try:\n            # 示例: 批量API调用\n            # 获取需要查询的参数(如订单号、用户ID等)\n            # query_params = batch_data['your_query_field'].unique().tolist()\n            \n            {% if cookiecutter.enable_cache == 'y' %}# 使用缓存方法\n            # return self._fetch_api_data_cached(tuple(query_params)){% else %}# 直接API调用\n            # api_url = 'https://api.example.com/your-endpoint'\n            # headers = {'Content-Type': 'application/json'}\n            # payload = json.dumps(query_params)\n            # response = requests.post(api_url, headers=headers, data=payload, timeout=30)\n            # \n            # if response.status_code == 200:\n            #     return response.json()\n            # else:\n            #     self.logger.warning(f\"API调用失败: {response.status_code}\")\n            #     return {}{% endif %}\n            \n            # TODO: 替换为你的实际API调用逻辑\n            return {}\n            \n        except Exception as e:\n            self.logger.error(f\"API调用异常: {str(e)}\")\n            return {}\n    \n    def process_business_logic(self, batch_data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        处理业务逻辑 - 批量处理\n        \n        Args:\n            batch_data: 当前批次的数据DataFrame\n            \n        Returns:\n            处理后的DataFrame，必须包含结果字段的值\n            \n        TODO: 实现你的核心业务处理逻辑\n        \"\"\"\n        try:\n            # 如果需要调用外部API，可以取消下面这行注释\n            # external_data = self.fetch_external_data(batch_data)\n            \n            # 处理每一行或批量处理\n            for idx, row in batch_data.iterrows():\n                # TODO: 实现你的业务逻辑\n                # 示例业务逻辑\n                # user_id = row['user_id']\n                # order_number = row['order_number']\n                \n                # 执行计算、比较、判断等业务逻辑\n                # result1_value = some_calculation(row)\n                # result2_value = some_comparison(row, external_data)\n                # result3_value = some_other_logic(row)\n                \n                # 设置结果值 (替换为你的实际逻辑)\n                batch_data.loc[idx, 'result1'] = ''  # TODO: 设置实际的结果值\n                batch_data.loc[idx, 'result2'] = ''  # TODO: 设置实际的结果值\n                batch_data.loc[idx, 'result3'] = ''  # TODO: 设置实际的结果值\n            \n            return batch_data\n            \n        except Exception as e:\n            self.logger.error(f\"业务逻辑处理异常: {str(e)}\")\n            # 返回原始数据，结果字段为空\n            for field in self.define_schema()['result_fields']:\n                batch_data[field] = ''\n            return batch_data"
  },
  {
   "cell_type": "markdown",
   "id": "execution",
   "metadata": {},
   "source": "## 3. 执行批处理\n\n### 调试模式 (推荐)\n在开发阶段，建议先使用调试模式测试业务逻辑，避免处理大量数据时出错："
  },
  {
   "cell_type": "markdown",
   "id": "mlteeq8g8oe",
   "source": "### 生产模式\n确认业务逻辑正确后，处理全部数据：",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "owked0muius",
   "source": "# 生产模式: 处理全部数据\n# processor = {{ cookiecutter.project_slug.title().replace('_', '') }}Processor(\n#     batch_size=DB_CONFIG['batch_size'],\n#     table_name=DB_CONFIG['table_name'], \n#     db_name=DB_CONFIG['db_name'],\n#     max_retries=DB_CONFIG['max_retries']\n# )\n# processor.run()  # 不传参数，处理全部数据",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-processor",
   "metadata": {},
   "outputs": [],
   "source": "# 创建处理器实例\nprocessor = {{ cookiecutter.project_slug.title().replace('_', '') }}Processor(\n    batch_size=DB_CONFIG['batch_size'],\n    table_name=DB_CONFIG['table_name'], \n    db_name=DB_CONFIG['db_name'],\n    max_retries=DB_CONFIG['max_retries']\n)\n\n# 🔧 调试模式: 只处理2个批次，用于测试业务逻辑\nprocessor.run(debug_batch_times=2)\n\n# ⚠️ 确认业务逻辑正确后，使用下面的命令处理全部数据:\n# processor.run()"
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## 4. 查看处理结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取处理统计\n",
    "stats = processor.get_statistics()\n",
    "print(\"处理统计:\")\n",
    "print(f\"总记录数: {stats['total']}\")\n",
    "print(f\"已处理: {stats['processed']}\")\n",
    "print(f\"待处理: {stats['pending']}\")\n",
    "print(f\"处理失败: {stats['failed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看处理结果样本\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(DB_CONFIG['db_name'])\n",
    "sample_df = pd.read_sql(f\"SELECT * FROM {DB_CONFIG['table_name']} WHERE is_processed = 1 LIMIT 10\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"处理结果样本:\")\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export",
   "metadata": {},
   "source": [
    "## 5. 导出结果 (可选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出处理结果\n",
    "output_file = '{{ cookiecutter.project_slug }}_results.csv'\n",
    "processor.export_results(output_file, only_processed=True)\n",
    "print(f\"结果已导出到: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}